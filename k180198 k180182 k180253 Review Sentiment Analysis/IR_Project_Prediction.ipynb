{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python393jvsc74a57bd0a5fdfbe66a03ff9212ea22d9271ad2ae0df4c7ce5c28013de70ce9e8aa13b69b",
   "display_name": "Python 3.9.3 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "a5fdfbe66a03ff9212ea22d9271ad2ae0df4c7ce5c28013de70ce9e8aa13b69b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textstat\n",
    "import joblib\n",
    "from nltk.stem import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flesch_reading_ease(text):\n",
    "    # formula=206.835-1.015(total_words/1)-84.6(syllables/total_words)\n",
    "    syllables=textstat.syllable_count(text)\n",
    "    words=textstat.lexicon_count(text, removepunct=True)\n",
    "    score=round(206.835-1.015*(words/1)-84.6*(float(syllables/words)),2)\n",
    "    # print(score)\n",
    "    return score\n",
    "\n",
    "def flesch_kincaid_grade_level(text):\n",
    "    # formula=0.39*(total_words/1)+11.8(syllables/total_words)-15.59\n",
    "    syllables=textstat.syllable_count(text)\n",
    "    words=textstat.lexicon_count(text, removepunct=True)\n",
    "    score=round(0.39*(words/1)+11.8*(syllables/words)-15.59,2)\n",
    "    # print(score)\n",
    "    return score\n",
    "\n",
    "#Remove extra white spaces, urls , mentions\n",
    "def preprocess(text):\n",
    "    text=text.lower()   \n",
    "    # print(stopwords)\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "    '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    parsed_text = re.sub('#\\S+', '', parsed_text)  # remove hashtags\n",
    "    parsed_text = re.sub('@\\S+', '  ', parsed_text)  # remove mentions\n",
    "    return parsed_text\n",
    "\n",
    "def cleaning(text):\n",
    "    text=preprocess(text)\n",
    "    return text\n",
    "def tokenization_with_stemming(text):\n",
    "    stemmer=PorterStemmer()\n",
    "    tokens = [stemmer.stem(t) for t in text.split()]\n",
    "    # print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def features(text):\n",
    "    sentiment_analyzer=VS()\n",
    "    sentiment = sentiment_analyzer.polarity_scores(text)\n",
    "    \n",
    "    words = preprocess(text) #Get text only\n",
    "    syllables = textstat.syllable_count(words)\n",
    "    num_chars = sum(len(w) for w in words)\n",
    "    num_chars_total = len(text)\n",
    "    num_terms = len(text.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = flesch_kincaid_grade_level(text)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = flesch_reading_ease(text)\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound']]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", text.lower())).strip()\n",
    "    return text.split()\n",
    "\n",
    "def get_feature_array(text):\n",
    "    feats=[]\n",
    "    for t in text:\n",
    "        feats.append(features(t))\n",
    "    return np.array(feats)    \n",
    "tfidf_vector = TfidfVectorizer(\n",
    "    tokenizer=tokenization_with_stemming,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=1500,\n",
    "    lowercase=True,\n",
    "     token_pattern='[a-zA-Z0-9]+',\n",
    "     strip_accents='unicode'\n",
    "    )\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None,\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=1500,lowercase=True,\n",
    "     token_pattern='[a-zA-Z0-9]+',\n",
    "     strip_accents='unicode'\n",
    "    )\n",
    "\n",
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syllables_per_sent\", \"num_chars\", \"num_chars_total\",\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \n",
    "                        \"vader compound\"]\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['You are so great', 'This is not worth a damn', 'I hate you', 'This is not worth it']\n"
     ]
    }
   ],
   "source": [
    "text=['You are so great','This is not worth a damn','I hate you', 'This is not worth it']\n",
    "foldername=r'asset Tweet'\n",
    "for t in text:\n",
    "    t=cleaning(t)\n",
    "# text=tokenization_with_stemming(text)\n",
    "print(text)\n",
    "# X=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LogisticRegression(class_weight='balanced', solver='newton-cg')\n"
     ]
    }
   ],
   "source": [
    "# Hatebase Model\n",
    "import pathlib\n",
    "path = os.getcwd()\n",
    "trained_model = joblib.load(path + '\\\\' + '{foldername}\\\\trained_model_logistic'.format(foldername=foldername))\n",
    "print(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4, 2399)\n",
      "C:\\Users\\umaim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "with open(path + '\\\\' + '{foldername}\\\\tfidf'.format(foldername=foldername), 'rb') as tfidf:\n",
    "  tfidf_vector = pickle.load(tfidf)\n",
    "with open(path + '\\\\' + '{foldername}\\\\pos'.format(foldername=foldername), 'rb') as tfidf:\n",
    "    pos_vector = pickle.load(tfidf)\n",
    "tfidf=tfidf_vector.transform(text).toarray()\n",
    "features=get_feature_array(text)\n",
    "\n",
    "# Get parts of speech tags for text and save as a string\n",
    "text_tag = []\n",
    "for t in text:\n",
    "    tokens = tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    text_tag.append(tag_str)\n",
    "\n",
    "pos = pos_vector.transform(pd.Series(text_tag)).toarray()\n",
    "\n",
    "M = np.concatenate([tfidf,pos,features],axis=1)\n",
    "\n",
    "X=pd.DataFrame(M)\n",
    "print(X.shape)\n",
    "# print(tfidf_vector)\n",
    "# print(pos_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TfidfVectorizer(decode_error='replace', max_features=1500, ngram_range=(1, 3),\n                norm=None,\n                preprocessor=<function preprocess at 0x0000025B4BFF3E50>,\n                stop_words='english', strip_accents='unicode',\n                sublinear_tf=True, token_pattern='[a-zA-Z0-9]+',\n                tokenizer=<function tokenization_with_stemming at 0x0000025B4BFF3B80>)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "trained_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  0 - hate speech\n",
    "#   1 - offensive  language\n",
    "#   2 - neither\n"
   ]
  }
 ]
}