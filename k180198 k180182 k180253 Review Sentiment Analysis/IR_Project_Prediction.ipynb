{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python380jvsc74a57bd01baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253",
   "display_name": "Python 3.8.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textstat\n",
    "import joblib\n",
    "from nltk.stem import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flesch_reading_ease(text):\n",
    "    # formula=206.835-1.015(total_words/1)-84.6(syllables/total_words)\n",
    "    syllables=textstat.syllable_count(text)\n",
    "    words=textstat.lexicon_count(text, removepunct=True)\n",
    "    score=round(206.835-1.015*(words/1)-84.6*(float(syllables/words)),2)\n",
    "    # print(score)\n",
    "    return score\n",
    "\n",
    "def flesch_kincaid_grade_level(text):\n",
    "    # formula=0.39*(total_words/1)+11.8(syllables/total_words)-15.59\n",
    "    syllables=textstat.syllable_count(text)\n",
    "    words=textstat.lexicon_count(text, removepunct=True)\n",
    "    score=round(0.39*(words/1)+11.8*(syllables/words)-15.59,2)\n",
    "    # print(score)\n",
    "    return score\n",
    "\n",
    "#Remove extra white spaces, urls , mentions\n",
    "def preprocess(text):\n",
    "    text=text.lower()   \n",
    "    # print(stopwords)\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "    '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    parsed_text = re.sub('#\\S+', '', parsed_text)  # remove hashtags\n",
    "    parsed_text = re.sub('@\\S+', '  ', parsed_text)  # remove mentions\n",
    "    return parsed_text\n",
    "\n",
    "def cleaning(text):\n",
    "    text=preprocess(text)\n",
    "    return text\n",
    "def tokenization_with_stemming(text):\n",
    "    stemmer=PorterStemmer()\n",
    "    tokens = [stemmer.stem(t) for t in text.split()]\n",
    "    # print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def features(text):\n",
    "    sentiment_analyzer=VS()\n",
    "    sentiment = sentiment_analyzer.polarity_scores(text)\n",
    "    \n",
    "    words = preprocess(text) #Get text only\n",
    "    syllables = textstat.syllable_count(words)\n",
    "    num_chars = sum(len(w) for w in words)\n",
    "    num_chars_total = len(text)\n",
    "    num_terms = len(text.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = flesch_kincaid_grade_level(text)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = flesch_reading_ease(text)\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound']]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", text.lower())).strip()\n",
    "    return text.split()\n",
    "\n",
    "def get_feature_array(text):\n",
    "    feats=[]\n",
    "    for t in text:\n",
    "        feats.append(features(t))\n",
    "    return np.array(feats)    \n",
    "tfidf_vector = TfidfVectorizer(\n",
    "    tokenizer=tokenization_with_stemming,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=1500,\n",
    "    lowercase=True,\n",
    "     token_pattern='[a-zA-Z0-9]+',\n",
    "     strip_accents='unicode'\n",
    "    )\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None,\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=1500,lowercase=True,\n",
    "     token_pattern='[a-zA-Z0-9]+',\n",
    "     strip_accents='unicode'\n",
    "    )\n",
    "\n",
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syllables_per_sent\", \"num_chars\", \"num_chars_total\",\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \n",
    "                        \"vader compound\"]\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['You are so great', 'This is not worth a damn', 'I hate you']\n"
     ]
    }
   ],
   "source": [
    "text=['You are so great','This is not worth a damn','I hate you']\n",
    "foldername='asset Tweet'\n",
    "for t in text:\n",
    "    t=cleaning(t)\n",
    "# text=tokenization_with_stemming(text)\n",
    "print(text)\n",
    "# X=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LogisticRegression(class_weight='balanced', solver='newton-cg')\n"
     ]
    }
   ],
   "source": [
    "# Hatebase Model\n",
    "trained_model = joblib.load(r'F:\\IR_Project\\k180198 k180182 k180253 Review Sentiment Analysis\\{foldername}\\trained_model_logistic'.format(foldername=foldername))\n",
    "print(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3, 2399)\n",
      "C:\\Program Files\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "with open(r'F:\\IR_Project\\k180198 k180182 k180253 Review Sentiment Analysis\\{foldername}\\tfidf'.format(foldername=foldername), 'rb') as tfidf:\n",
    "  tfidf_vector = pickle.load(tfidf)\n",
    "with open(r'F:\\IR_Project\\k180198 k180182 k180253 Review Sentiment Analysis\\{foldername}\\pos'.format(foldername=foldername), 'rb') as tfidf:\n",
    "    pos_vector = pickle.load(tfidf)\n",
    "tfidf=tfidf_vector.transform(text).toarray()\n",
    "features=get_feature_array(text)\n",
    "\n",
    "# Get parts of speech tags for text and save as a string\n",
    "text_tag = []\n",
    "for t in text:\n",
    "    tokens = tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    text_tag.append(tag_str)\n",
    "\n",
    "pos = pos_vector.transform(pd.Series(text_tag)).toarray()\n",
    "\n",
    "M = np.concatenate([tfidf,pos,features],axis=1)\n",
    "\n",
    "X=pd.DataFrame(M)\n",
    "print(X.shape)\n",
    "# print(tfidf_vector)\n",
    "# print(pos_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TfidfVectorizer(decode_error='replace', max_features=1500, ngram_range=(1, 3),\n                norm=None,\n                preprocessor=<function preprocess at 0x000002A442D92790>,\n                stop_words='english', strip_accents='unicode',\n                sublinear_tf=True, token_pattern='[a-zA-Z0-9]+',\n                tokenizer=<function tokenization_with_stemming at 0x000002A442D92670>)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2, 1, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "trained_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  0 - hate speech\n",
    "#   1 - offensive  language\n",
    "#   2 - neither\n"
   ]
  }
 ]
}