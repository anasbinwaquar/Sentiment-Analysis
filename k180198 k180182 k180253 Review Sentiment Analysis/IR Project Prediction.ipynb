{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python380jvsc74a57bd01baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253",
   "display_name": "Python 3.8.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import seaborn\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textstat\n",
    "from nltk.stem import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flesch_reading_ease(text):\n",
    "    # formula=206.835-1.015(total_words/1)-84.6(syllables/total_words)\n",
    "    syllables=textstat.syllable_count(text)\n",
    "    words=textstat.lexicon_count(text, removepunct=True)\n",
    "    score=round(206.835-1.015*(words/1)-84.6*(float(syllables/words)),2)\n",
    "    # print(score)\n",
    "    return score\n",
    "\n",
    "def flesch_kincaid_grade_level(text):\n",
    "    # formula=0.39*(total_words/1)+11.8(syllables/total_words)-15.59\n",
    "    syllables=textstat.syllable_count(text)\n",
    "    words=textstat.lexicon_count(text, removepunct=True)\n",
    "    score=round(0.39*(words/1)+11.8*(syllables/words)-15.59,2)\n",
    "    # print(score)\n",
    "    return score\n",
    "\n",
    "def total_characters(text):\n",
    "    count=0\n",
    "    for char in text:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "#Remove extra white spaces, urls , mentions\n",
    "def preprocess(text):\n",
    "    text=text.lower()   \n",
    "    # print(stopwords)\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "    '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    parsed_text = re.sub('#\\S+', '', parsed_text)  # remove hashtags\n",
    "    parsed_text = re.sub('@\\S+', '  ', parsed_text)  # remove mentions\n",
    "    return parsed_text\n",
    "\n",
    "def cleaning(text):\n",
    "    text=preprocess(text)\n",
    "    return text\n",
    "def tokenization_with_stemming(text):\n",
    "    stemmer=PorterStemmer()\n",
    "    tokens = [stemmer.stem(t) for t in text.split()]\n",
    "    # print(tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", text.lower())).strip()\n",
    "    return text.split()\n",
    "\n",
    "def set_class(text):\n",
    "    if text=='NAG':\n",
    "        return 2\n",
    "    elif text=='OAG':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def preprocessing():\n",
    "    path = pathlib.Path(__file__).parent.absolute()\n",
    "    path = str(path)\n",
    "    #Loading file set to data frame\n",
    "    df=pd.read_csv(path + \"\\\\trac-gold-set\\\\agr_en_fb_gold.csv\", encoding = \"utf-8\")\n",
    "    #Case folding and porter stemming\n",
    "    # df[\"preprocess\"]=df[\"text\"].apply(lambda x:x.lower())\n",
    "    df[\"preprocess\"]=df[\"text\"].apply(lambda x:preprocess(x))\n",
    "    # print(df['preprocess'])\n",
    "    df[\"flesch_reading_ease\"]=df[\"text\"].apply(lambda x:flesch_reading_ease(x))\n",
    "    df[\"flesch_kincaid_grade_level\"]=df[\"text\"].apply(lambda x:flesch_kincaid_grade_level(x))\n",
    "    df[\"syllables\"]=df[\"text\"].apply(lambda x:textstat.syllable_count(x))\n",
    "    df[\"words\"]=df[\"text\"].apply(lambda x:textstat.lexicon_count(x))\n",
    "    df[\"characters\"]=df[\"text\"].apply(lambda x:total_characters(x))\n",
    "    df[\"class\"]=df[\"aggresssion-level\"].apply(lambda x:set_class(x))\n",
    "    #Export processed CSV\n",
    "    # print(df.text)\n",
    "    df.to_csv('processed_data.csv')\n",
    "    # df.describe()\n",
    "\n",
    "def features(text):\n",
    "    sentiment_analyzer=VS()\n",
    "    sentiment = sentiment_analyzer.polarity_scores(text)\n",
    "    \n",
    "    words = preprocess(text) #Get text only\n",
    "    syllables = textstat.syllable_count(words)\n",
    "    num_chars = sum(len(w) for w in words)\n",
    "    num_chars_total = len(text)\n",
    "    num_terms = len(text.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = flesch_kincaid_grade_level(text)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = flesch_reading_ease(text)\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound']]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(text):\n",
    "    feats=[]\n",
    "    for t in text:\n",
    "        feats.append(features(t))\n",
    "    return np.array(feats)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "tfidf_vector = TfidfVectorizer(\n",
    "    tokenizer=tokenization_with_stemming,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=1500,\n",
    "    lowercase=True,\n",
    "     token_pattern='[a-zA-Z0-9]+',\n",
    "     strip_accents='unicode'\n",
    "    )\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None,\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=1500,lowercase=True,\n",
    "     token_pattern='[a-zA-Z0-9]+',\n",
    "     strip_accents='unicode'\n",
    "    \n",
    "    )\n",
    "\n",
    "def processing(text):\n",
    "    tfidf = vectorizer.fit_transform(text).toarray()\n",
    "    features=get_feature_array(text)\n",
    "\n",
    "    #Get parts of speech tags for text and save as a string\n",
    "    text_tag = []\n",
    "    for t in text:\n",
    "        tokens = tokenize(preprocess(t))\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        tag_list = [x[1] for x in tags]\n",
    "        tag_str = \" \".join(tag_list)\n",
    "        text_tag.append(tag_str)\n",
    "\n",
    "    pos = pos_vectorizer.fit_transform(pd.Series(text_tag)).toarray()\n",
    "\n",
    "    M = np.concatenate([tfidf,pos,features],axis=1)\n",
    "    return M\n",
    "\n",
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syllables_per_sent\", \"num_chars\", \"num_chars_total\",\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \n",
    "                        \"vader compound\"]\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['@sardanarohit :While entire nation is praying for our Siachin Brave heart #JNU pays tribute to a terrorist. #ShutDownJNU #JNUAfzalTribute']\n"
     ]
    }
   ],
   "source": [
    "text=['@sardanarohit :While entire nation is praying for our Siachin Brave heart #JNU pays tribute to a terrorist. #ShutDownJNU #JNUAfzalTribute']\n",
    "for t in text:\n",
    "    t=cleaning(t)\n",
    "# text=tokenization_with_stemming(text)\n",
    "print(text)\n",
    "# X=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LogisticRegression(C=0.01, class_weight='balanced', solver='newton-cg')\n"
     ]
    }
   ],
   "source": [
    "# Hatebase Model\n",
    "trained_model = joblib.load(r'F:\\IR_Project\\k180198 k180182 k180253 Review Sentiment Analysis\\asset Tweet set hatebase\\trained_model_logistic')\n",
    "print(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uni Dataset Mode\n",
    "trained_model = joblib.load(r'F:\\IR_Project\\k180198 k180182 k180253 Review Sentiment Analysis\\asset Uni dataset\\trained_model_logistic')\n",
    "print(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 13)\n",
      "C:\\Program Files\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "with open(r'F:\\IR_Project\\k180198 k180182 k180253 Review Sentiment Analysis\\asset\\tfidf', 'rb') as tfidf:\n",
    "  tfidf_vector = pickle.load(tfidf)\n",
    "with open(r'F:\\IR_Project\\k180198 k180182 k180253 Review Sentiment Analysis\\asset\\pos', 'rb') as tfidf:\n",
    "    pos_vector = pickle.load(tfidf)\n",
    "tfidf=tfidf_vector.transform(text).toarray()\n",
    "features=get_feature_array(text)\n",
    "\n",
    "# Get parts of speech tags for text and save as a string\n",
    "text_tag = []\n",
    "for t in text:\n",
    "    tokens = tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    text_tag.append(tag_str)\n",
    "\n",
    "pos = pos_vector.transform(pd.Series(text_tag)).toarray()\n",
    "\n",
    "M = np.concatenate([tfidf,pos,features],axis=1)\n",
    "\n",
    "X=pd.DataFrame(M)\n",
    "print(features.shape)\n",
    "# print(tfidf_vector)\n",
    "# print(pos_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TfidfVectorizer(decode_error='replace', max_features=1500, ngram_range=(1, 3),\n                norm=None,\n                preprocessor=<function preprocess at 0x00000176D5D3E550>,\n                stop_words='english', strip_accents='unicode',\n                sublinear_tf=True, token_pattern='[a-zA-Z0-9]+',\n                tokenizer=<function tokenization_with_stemming at 0x00000176D5D3E4C0>)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "trained_model.predict(X)"
   ]
  }
 ]
}